### 问题描述：
<p>，代码没报错，运行无结果，debug有问题</p>
自己编写的一段简单爬虫，代码运行之后显示
Process finished with exit code 0，，，就是没有结果
debug显示如下Connected to pydev debugger (build 203.7717.65)
网络上的pydt试过，删除.idea试过，版本也跟新了，个人认为应该是main（）函数问题，但是不知道哪里出错了，希望来个大佬指点一下下面是代码


```python
import urllib.request,urllib.parse,urllib.error
from bs4 import BeautifulSoup as bs

def main():
    #爬取的网址
    url="http://data.eastmoney.com/cjsj/gdp.html"

    headers={    #模拟浏览器头部
        "User-Agent":"Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 88.0"
        ".4324.182Safari / 537.36"
    }




def get_data():
    datalist=[]
    for i in range(0,3):
        pass
    soup=bs(html,"html.parse")
    print(soup.title)


def askURL(url):
    req=urllib.request.Request(url=url,headers=headers)
    html=""
    try:
        response=urllib.request.urlopen(req)
        html=response.read().decode("utf-8")
        print(html)
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)

    return html


if __name__ == '__main()__':
    askURL(url)
    get_data() 
```

### 修改方案：
url和headers是拿出来，还有main写错了，具体代码如下所示：


```python
import urllib.request,urllib.parse,urllib.error
from bs4 import BeautifulSoup as bs

url="http://data.eastmoney.com/cjsj/gdp.html"
 
headers={    #模拟浏览器头部
    "User-Agent":"Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 88.0"
    ".4324.182Safari / 537.36"
}

 
def get_data():
    datalist=[]
    for i in range(0,3):
        pass
    soup=bs(html,"html.parse")
    print(soup.title)
 
 
def askURL(url):
    req=urllib.request.Request(url=url,headers=headers)
    html=""
    try:
        response=urllib.request.urlopen(req)
        html=response.read().decode("utf-8")
        print(html)
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
 
    return html

if __name__ == '__main__':
    askURL(url)
    get_data()

```
 
 
### 人工打分：
完美解决问题
