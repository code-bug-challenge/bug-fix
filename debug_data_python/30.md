### 问题描述：
<p>小白Python爬虫运行程序发生异常，求指教！</p>
我在B站上跟着视频教程学习网页爬虫，做到了想要爬取网页上的演员信息的时候就出错了，以下是目前的代码


```python
import bs4
import re
import urllib.request,urllib.error
import sqlite3
from bs4 import BeautifulSoup
import sys
import requests
import importlib
importlib.reload(sys)


def main():
#爬取页面
    baseurl="https://baike.baidu.com/item/平凡的荣耀"
    datalist=getData(baseurl)
    askURL("https://baike.baidu.com/item/平凡的荣耀")
#savepath=".\\平凡的荣耀数据.json"
#saveData(savepath)


#解析页面
#保存页面

def askURL(url):#获取指定页面内容
    head={
        "User-Agent":"Mozilla/5.0(Windows NT 10.0;WOW64)AppleWebKit/537.36(KHTML, likeGecko)Chrome/70.0.3538.25Safari/537.36Core/1.70.3741.400QQBrowser/10.5.3863.400"
    }#假装自己是一个浏览器
    request=urllib.request.Request(url,headers=head)
    html=""
    try:
        response = requests.get(url,headers=head)
        html=BeautifulSoup(response.text,'html')
        #print(html)
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
    return html

def getData(url):#爬取内容
    datalist=[]
    return datalist

def getTVData(url):
    datalist=[]
    html=askURL(url)
    soup=BeautifulSoup(html,"html.parser")
    for item in soup.find_all('div',class_="viewport"):#收视率
        print(item)
    return
    #return datalist

def saveData(savepath):
    print('save...')

# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    baseurl="https://baike.baidu.com/item/平凡的荣耀"
    getTVData(baseurl)
    



```
结果报错了TypeError: 'NoneType' object is not callable
求大佬指教…… 
### 修改方案：


```python
import bs4
	
import re
	
import urllib.request,urllib.error
	
import sqlite3
	
from bs4 import BeautifulSoup
	
import sys
	
import requests
	
import importlib
	
importlib.reload(sys)
	
 
	
def main():
	
#爬取页面
	
	baseurl="https://baike.baidu.com/item/平凡的荣耀"
	
	datalist=getData(baseurl)
	
	askURL("https://baike.baidu.com/item/平凡的荣耀")
	
#savepath=".\\平凡的荣耀数据.json"
	
#saveData(savepath)
	
 
	
 
	
#解析页面
	
#保存页面
	
 
	
def askURL(url):#获取指定页面内容
	
	head={
	
		"User-Agent":"Mozilla/5.0(Windows NT 10.0;WOW64)AppleWebKit/537.36(KHTML, likeGecko)Chrome/70.0.3538.25Safari/537.36Core/1.70.3741.400QQBrowser/10.5.3863.400"
	
	}#假装自己是一个浏览器
	
	request=urllib.request.Request(url,headers=head)
	
	html=""
	
	try:
	
		response = requests.get(url,headers=head)
	
		html=BeautifulSoup(response.text,'lxml')
	
		#print(html)
	
	except urllib.error.URLError as e:
	
		if hasattr(e,"code"):
	
			print(e.code)
	
		if hasattr(e,"reason"):
	
			print(e.reason)
	
	return html
	
 
	
def getData(url):#爬取内容
	
	datalist=[]
	
	return datalist
	
 
	
def getTVData(url):
	
	datalist=[]
	

	soup=askURL(url)
	
	for item in soup.find_all('div',class_="viewport"):#收视率
	
		print(item)
	
	return
	
	#return datalist
	
 
	
def saveData(savepath):
	
	print('save...')
	
 
	
# Press the green button in the gutter to run the script.
	
if __name__ == '__main__':
	
	baseurl="https://baike.baidu.com/item/平凡的荣耀"
	
	getTVData(baseurl)

```
askURL返回的就是BeautifulSoup对象了，getTVData函数里改成soup=askURL(url)即可
### 人工打分：
完美解决问题
